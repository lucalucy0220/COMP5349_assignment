{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-478304718310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import concat, col, lit, size, explode, monotonically_increasing_id, split, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, VectorAssembler, PCA, Normalizer\n",
    "from pyspark.ml.clustering import KMeans, LDA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b694387bc921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Assignment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Assignment\") \\\n",
    "    .getOrCreate()\n",
    "sqlContext = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_matched_path = \"s3://mnliassignment/dev_matched.tsv\"\n",
    "dev_mismatched_path = \"s3://mnliassignment/dev_mismatched.tsv\"\n",
    "test_matched_path = \"s3://mnliassignment/test_matched.tsv\"\n",
    "test_mismatched_path = \"s3://mnliassignment/test_mismatched.tsv\"\n",
    "train_path = \"s3://mnliassignment/train.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schema for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([\n",
    "    StructField(\"index\", IntegerType(), True),\n",
    "    StructField(\"promptID\", IntegerType(), True),\n",
    "    StructField(\"pairID\", StringType(), True),\n",
    "    StructField(\"genre\", StringType(), True),\n",
    "    StructField(\"sentence1_binary_parse\", StringType(), True),\n",
    "    StructField(\"sentence2_binary_parse\", StringType(), True),\n",
    "    StructField(\"sentence1_parse\", StringType(), True),\n",
    "    StructField(\"sentence2_parse\", StringType(), True),\n",
    "    StructField(\"sentence1\", StringType(), True),\n",
    "    StructField(\"sentence2\", StringType(), True),\n",
    "    StructField(\"label1\", StringType(), True),\n",
    "    StructField(\"label2\", StringType(), True),\n",
    "    StructField(\"label3\", StringType(), True),\n",
    "    StructField(\"label4\", StringType(), True),\n",
    "    StructField(\"label5\", StringType(), True),\n",
    "    StructField(\"gold_label\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_matched_df= sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"delimiter\", \"\\t\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .schema(data_schema)\\\n",
    "                    .load(dev_matched_path)\\\n",
    "                    .repartition(5)\\\n",
    "                    .select(\"genre\", \"sentence1\", \"sentence2\")\n",
    "\n",
    "dev_mismatched_df= sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"delimiter\", \"\\t\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .schema(data_schema)\\\n",
    "                    .load(dev_mismatched_path)\\\n",
    "                    .repartition(5)\\\n",
    "                    .select(\"genre\", \"sentence1\", \"sentence2\")\n",
    "\n",
    "test_matched_df= sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"delimiter\", \"\\t\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .schema(data_schema)\\\n",
    "                    .load(test_matched_path)\\\n",
    "                    .repartition(5)\\\n",
    "                    .select(\"genre\", \"sentence1\", \"sentence2\")\n",
    "\n",
    "test_mismatched_df= sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"delimiter\", \"\\t\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .schema(data_schema)\\\n",
    "                    .load(test_mismatched_path)\\\n",
    "                    .repartition(5)\\\n",
    "                    .select(\"genre\", \"sentence1\", \"sentence2\")\n",
    "\n",
    "train_df= sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"delimiter\", \"\\t\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .schema(data_schema)\\\n",
    "                    .load(train_path)\\\n",
    "                    .repartition(5)\\\n",
    "                    .select(\"genre\", \"sentence1\", \"sentence2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Vocab Exploration - Matched and Mismatched Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer to split the sentence to list of words\n",
    "tokenizer = Tokenizer(inputCol=\"combine_sentence\", outputCol=\"words\")\n",
    "\n",
    "#dev_mismatched\n",
    "dev_mismatched_combine_sentence = dev_mismatched_df.select(concat(col(\"sentence1\"), lit(\" \"), col(\"sentence2\")).alias(\"combine_sentence\"))\n",
    "dev_mismatched_transform = tokenizer.transform(dev_mismatched_combine_sentence.na.fill({'combine_sentence': 'the' })).filter(size('words') > 0)\n",
    "\n",
    "#test_mismatched\n",
    "test_mismatched_combine_sentence = test_mismatched_df.select(concat(col(\"sentence1\"), lit(\" \"), col(\"sentence2\")).alias(\"combine_sentence\"))\n",
    "test_mismatched_transform = tokenizer.transform(test_mismatched_combine_sentence.na.fill({'combine_sentence': 'the' })).filter(size('words') > 0)\n",
    "\n",
    "#dev_matched\n",
    "dev_matched_combine_sentence = dev_matched_df.select(concat(col(\"sentence1\"), lit(\" \"), col(\"sentence2\")).alias(\"combine_sentence\"))\n",
    "dev_matched_transform = tokenizer.transform(dev_matched_combine_sentence.na.fill({'combine_sentence': 'the' })).filter(size('words') > 0)\n",
    "\n",
    "#test_matched\n",
    "test_matched_combine_sentence = test_matched_df.select(concat(col(\"sentence1\"), lit(\" \"), col(\"sentence2\")).alias(\"combine_sentence\"))\n",
    "test_matched_transform = tokenizer.transform(test_matched_combine_sentence.na.fill({'combine_sentence': 'the' })).filter(size('words') > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_match = dev_matched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "test_match = test_matched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "match_concat = dev_match.union(test_match).distinct()\n",
    "\n",
    "dev_mismatch = dev_mismatched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "test_mismatch = test_mismatched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "mismatch_concat = dev_mismatch.union(test_mismatch).distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_match = dev_matched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "test_match = test_matched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "match_concat = dev_match.union(test_match).distinct()\n",
    "\n",
    "dev_mismatch = dev_mismatched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "test_mismatch = test_mismatched_transform.select(explode(\"words\").alias(\"words\")).distinct()\n",
    "mismatch_concat = dev_mismatch.union(test_mismatch).distinct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 number of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = match_concat.intersect(mismatch_concat)\n",
    "intersect.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 unique words to matched sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_matched = match_concat.subtract(mismatch_concat)\n",
    "unique_matched.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 unique words to mismatched sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mismatched = mismatch_concat.subtract(match_concat)\n",
    "unique_mismatched.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Vocab Exploration - Training Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 percentage of words in 5,4,3,2,1 genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"combine_sentence\", outputCol=\"words\")\n",
    "\n",
    "train_sentence_genre = train_df \\\n",
    "                  .select(concat(col(\"sentence1\"), lit(\" \"), col(\"sentence2\")) \\\n",
    "                  .alias(\"combine_sentence\"), \"genre\")\n",
    "                  \n",
    "train_tokenized = tokenizer \\\n",
    "                  .transform(train_sentence_genre.na.fill({'combine_sentence': 'the' })) \\\n",
    "                  .filter(size('words') > 0).cache()\n",
    "\n",
    "train_word_genre = train_tokenized \\\n",
    "                  .select(explode(\"words\").alias(\"word\"), \"genre\") \\\n",
    "                  .distinct()\n",
    "\n",
    "train_word_appear = train_word_genre \\\n",
    "                  .groupBy('word').count() \\\n",
    "                  .select('word', col('count').alias('appear')).cache()\n",
    "\n",
    "train_five_genre = train_word_appear.filter(\"appear = 5\")\n",
    "train_four_genre = train_word_appear.filter(\"appear = 4\")\n",
    "train_three_genre = train_word_appear.filter(\"appear = 3\")\n",
    "train_two_genre = train_word_appear.filter(\"appear = 2\")\n",
    "train_one_genre = train_word_appear.filter(\"appear = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_count = train_word_appear.count()\n",
    "train_five_genre_count = train_five_genre.count()\n",
    "train_four_genre_count = train_four_genre.count()\n",
    "train_three_genre_count = train_three_genre.count()\n",
    "train_two_genre_count = train_two_genre.count()\n",
    "train_one_genre_count = train_one_genre.count()\n",
    "\n",
    "fivegenre = train_five_genre_count/train_all_count*100\n",
    "fourgenre = train_four_genre_count/train_all_count*100\n",
    "threegenre = train_three_genre_count/train_all_count*100\n",
    "twogenre = train_two_genre_count/train_all_count*100\n",
    "onegenre = train_one_genre_count/train_all_count*100\n",
    "\n",
    "print(\"Total number of words in training set: \"+ str(train_all_count))\n",
    "print(\"Percentage of words appearing in 5 genres: \"+ str(round(fivegenre, 3))+ \"%\")\n",
    "print(\"Percentage of words appearing in 4 genres: \"+ str(round(fourgenre, 3)) + \"%\")\n",
    "print(\"Percentage of words appearing in 3 genres: \"+ str(round(threegenre, 3)) + \"%\")\n",
    "print(\"Percentage of words appearing in 2 genres: \"+ str(round(twogenre, 3)) + \"%\")\n",
    "print(\"Percentage of words appearing in 1 genres: \"+ str(round(onegenre, 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 same percentage of words after removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(x):\n",
    "  punc='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "  j=[]\n",
    "  for i in x:\n",
    "    for ch in punc:\n",
    "      i = i.replace(ch, '')\n",
    "    j.append(i)\n",
    "  return j\n",
    "\n",
    "clean_str_udf = udf(lambda z: clean_str(z) if z is not None else None, ArrayType(StringType()))\n",
    "\n",
    "#stopword from CoreNLP\n",
    "stopwordList = ['!!', '?!', '??', '!?', '`', '``', \"''\", '-lrb-', '-rrb-', '-lsb-', '-rsb-', ',', '.', ':', ';', '\"', \"'\", '?', '<', '>', '{', '}', '[', ']', '+', '-', '(', ')', '&', '%', '$', '@', '!', '^', '#', '*', '..', '...', \"'ll\", \"'s\", \"'m\", 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', '###', 'return', 'arent', 'cant', 'couldnt', 'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'hes', 'heres', 'hows', 'im', 'isnt', 'its', 'lets', 'mustnt', 'shant', 'shes', 'shouldnt', 'thats', 'theres', 'theyll', 'theyre', 'theyve', 'wasnt', 'were', 'werent', 'whats', 'whens', 'wheres', 'whos', 'whys', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve']\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"combine_sentence\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stopwordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tokenized from 3.2.1\n",
    "\n",
    "train_remover = remover \\\n",
    "                .transform(train_tokenized) \\\n",
    "                .select('filtered', 'genre')\n",
    "\n",
    "train_cleaned = train_remover \\\n",
    "            .withColumn('words', clean_str_udf(train_remover.filtered)) \\\n",
    "            .select('words', 'genre')\n",
    "\n",
    "train_cleaned_word_genre = train_cleaned \\\n",
    "                        .select(explode('words').alias('word'), 'genre') \\\n",
    "                        .distinct()\n",
    "\n",
    "train_cleaned_word_appear = train_cleaned_word_genre \\\n",
    "                            .groupBy('word').count() \\\n",
    "                            .select('word', col('count').alias('appear')).cache()\n",
    "\n",
    "train_cleaned_five_genre = train_cleaned_word_appear.filter(\"appear = 5\")\n",
    "train_cleaned_four_genre = train_cleaned_word_appear.filter(\"appear = 4\")\n",
    "train_cleaned_three_genre = train_cleaned_word_appear.filter(\"appear = 3\")\n",
    "train_cleaned_two_genre = train_cleaned_word_appear.filter(\"appear = 2\")\n",
    "train_cleaned_one_genre = train_cleaned_word_appear.filter(\"appear = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_count = train_cleaned_word_appear.count()\n",
    "train_cleaned_five_genre_count = train_cleaned_five_genre.count() \n",
    "train_cleaned_four_genre_count = train_cleaned_four_genre.count() \n",
    "train_cleaned_three_genre_count = train_cleaned_three_genre.count() \n",
    "train_cleaned_two_genre_count = train_cleaned_two_genre.count() \n",
    "train_cleaned_one_genre_count = train_cleaned_one_genre.count() \n",
    "\n",
    "cleaned_fivegenre = train_cleaned_five_genre_count/train_cleaned_count*100\n",
    "cleaned_fourgenre = train_cleaned_four_genre_count/train_cleaned_count*100\n",
    "cleaned_threegenre = train_cleaned_three_genre_count/train_cleaned_count*100\n",
    "cleaned_twogenre = train_cleaned_two_genre_count/train_cleaned_count*100\n",
    "cleaned_onegenre = train_cleaned_one_genre_count/train_cleaned_count*100\n",
    "\n",
    "print(\"Total number of words in training set (after removing stopwords): \"+ str(train_cleaned_count))\n",
    "print(\"Percentage of words appearing in 5 genres: \"+ str(round(cleaned_fivegenre, 3)) + \"%\")\n",
    "print(\"Percentage of words appearing in 4 genres: \"+ str(round(cleaned_fourgenre, 3)) + \"%\")\n",
    "print(\"Percentage of words appearing in 3 genres: \"+ str(round(cleaned_threegenre, 3)) + \"%\")\n",
    "print(\"Percentage of words appearing in 2 genres: \"+ str(round(cleaned_twogenre, 3)) + \"%\")\n",
    "print(\"Percentage of words appearing in 1 genres: \"+ str(round(cleaned_onegenre, 3)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Sentence Vector Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 TD IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen1 = train_df.drop(\"sentence2\")\n",
    "sen2 = train_df.drop(\"sentence1\")\n",
    "combine_train_df = sen1.union(sen2).withColumnRenamed(\"sentence1\",\"sentence\")\n",
    "combine_train_df= combine_train_df.na.fill({'sentence': '' })\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# wordsData = tokenizer.transform(combine_train_df)\n",
    "\n",
    "#.filter(size('words') > 0)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "# featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "kmeans = KMeans(featuresCol='features',k=5, seed=23, maxIter=100)\n",
    "# model = kmeans.fit(rescaledData)\n",
    "# results = model.transform(rescaledData)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, kmeans])\n",
    "model = pipeline.fit(combine_train_df)\n",
    "results = model.transform(combine_train_df).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cluster label name\n",
    "results.filter(results[\"prediction\"]==0).groupBy(\"genre\").count().show()\n",
    "results.filter(results[\"prediction\"]==1).groupBy(\"genre\").count().show()\n",
    "results.filter(results[\"prediction\"]==2).groupBy(\"genre\").count().show()\n",
    "results.filter(results[\"prediction\"]==3).groupBy(\"genre\").count().show()\n",
    "results.filter(results[\"prediction\"]==4).groupBy(\"genre\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of the percentage\n",
    "# cluster label name by the maximum genre (duplicate allowed)\n",
    "'''\n",
    "from the above result, we compute the label name for each row:\n",
    "cluster 0 = fiction\n",
    "cluster 1 = government\n",
    "cluster 2 = telephone\n",
    "cluster 3 = telephone\n",
    "cluster 4 = telephone\n",
    "\n",
    "In the confusion matrix, the column is as followed:\n",
    "true label 0 = travel\n",
    "true label 1 = slate\n",
    "true label 2 = fiction\n",
    "true label 3 = government\n",
    "true label 4 = telephone\n",
    "'''\n",
    "# row cluster 0: total = 542038\n",
    "travel_0 = 108250 / 542038\n",
    "slate_0 = 118928 / 542038\n",
    "fiction_0 = 123198 / 542038\n",
    "gov_0 = 105072 / 542038\n",
    "tele_0 = 86590 / 542038\n",
    "\n",
    "# row cluster 1: total = 134066\n",
    "travel_1 = 46145 / 134066\n",
    "slate_1 = 27015 / 134066\n",
    "fiction_1 = 8432 / 134066\n",
    "gov_1 = 47880 / 134066\n",
    "tele_1 = 4594 / 134066\n",
    "\n",
    "# row cluster 2: total = 29488\n",
    "travel_2 = 21 / 29488\n",
    "slate_2 = 228 / 29488\n",
    "fiction_2 = 163 / 29488\n",
    "gov_2 = 58 / 29488\n",
    "tele_2 = 29018 / 29488\n",
    "\n",
    "# row cluster 3: total = 75165\n",
    "travel_3 = 284 / 75165\n",
    "slate_3 = 8441 / 75165\n",
    "fiction_3 = 22903 / 75165\n",
    "gov_3 = 58 / 75165\n",
    "tele_3 = 41847 / 75165\n",
    "\n",
    "# row cluster 4: total = 4647\n",
    "travel_4 = 0 / 4647\n",
    "slate_4 = 0 / 4647\n",
    "fiction_4 = 0 / 4647\n",
    "gov_4 = 0 / 4647\n",
    "tele_4 = 4647 / 4647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_embed(rev_text_partition):\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "    embed = hub.Module(module_url)\n",
    "    # mapPartition would supply element inside a partition using generator stype\n",
    "    # this does not fit tensorflow stype\n",
    "    rev_text_list = [text for text in rev_text_partition]\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(rev_text_list))\n",
    "    return message_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trabel df to rdd\n",
    "travel_rdd = combine_train_df.filter(combine_train_df[\"genre\"]==\"travel\")\\\n",
    "                                    .select('sentence').rdd.map(lambda row: str(row[0])).filter(lambda data: data is not None).cache()\n",
    "travel_embedding = travel_rdd.mapPartitions(review_embed).cache()\n",
    "\n",
    "#telephone df to rdd\n",
    "telephone_rdd = combine_train_df.filter(combine_train_df[\"genre\"]==\"telephone\")\\\n",
    "                                    .select('sentence').rdd.map(lambda row: str(row[0])).filter(lambda data: data is not None).cache()\n",
    "telephone_embedding = telephone_rdd.mapPartitions(review_embed).cache()\n",
    "\n",
    "#slate df to rdd\n",
    "slate_rdd = combine_train_df.filter(combine_train_df[\"genre\"]==\"slate\")\\\n",
    "                                    .select('sentence').rdd.map(lambda row: str(row[0])).filter(lambda data: data is not None).cache()\n",
    "slate_embedding = slate_rdd.mapPartitions(review_embed).cache()\n",
    "\n",
    "#government df to rdd\n",
    "government_rdd = combine_train_df.filter(combine_train_df[\"genre\"]==\"government\")\\\n",
    "                                    .select('sentence').rdd.map(lambda row: str(row[0])).filter(lambda data: data is not None).cache()\n",
    "government_embedding = government_rdd.mapPartitions(review_embed).cache()\n",
    "\n",
    "#fiction df to rdd\n",
    "fiction_rdd = combine_train_df.filter(combine_train_df[\"genre\"]==\"fiction\")\\\n",
    "                                    .select('sentence').rdd.map(lambda row: str(row[0])).filter(lambda data: data is not None).cache()\n",
    "fiction_embedding = fiction_rdd.mapPartitions(review_embed).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fiction embed\n",
    "fiction_embedding_df = spark.createDataFrame(fiction_embedding.map(lambda v: v.tolist()))\n",
    "fiction_assembler = VectorAssembler(inputCols=fiction_embedding_df.columns,outputCol=\"features\")\n",
    "fiction_embedding_vectors = fiction_assembler.transform(fiction_embedding_df).select(\"features\")\n",
    "\n",
    "#travel embed\n",
    "travel_embedding_df = spark.createDataFrame(travel_embedding.map(lambda v: v.tolist()))\n",
    "travel_assembler = VectorAssembler(inputCols=travel_embedding_df.columns,outputCol=\"features\")\n",
    "travel_embedding_vectors = travel_assembler.transform(travel_embedding_df).select(\"features\")\n",
    "\n",
    "#telephone embed\n",
    "tele_embedding_df = spark.createDataFrame(telephone_embedding.map(lambda v: v.tolist()))\n",
    "tele_assembler = VectorAssembler(inputCols=tele_embedding_df.columns,outputCol=\"features\")\n",
    "tele_embedding_vectors = tele_assembler.transform(tele_embedding_df).select(\"features\")\n",
    "\n",
    "#government embed\n",
    "gov_embedding_df = spark.createDataFrame(government_embedding.map(lambda v: v.tolist()))\n",
    "gov_assembler = VectorAssembler(inputCols=gov_embedding_df.columns,outputCol=\"features\")\n",
    "gov_embedding_vectors = gov_assembler.transform(gov_embedding_df).select(\"features\")\n",
    "\n",
    "#slate embed\n",
    "slate_embedding_df = spark.createDataFrame(slate_embedding.map(lambda v: v.tolist()))\n",
    "slate_assembler = VectorAssembler(inputCols=slate_embedding_df.columns,outputCol=\"features\")\n",
    "slate_embedding_vectors = slate_assembler.transform(slate_embedding_df).select(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol='features',k=5, seed=6)\n",
    "\n",
    "#travel\n",
    "travel_model = kmeans.fit(travel_embedding_vectors)\n",
    "travel_results = travel_model.transform(travel_embedding_vectors)\n",
    "\n",
    "#government\n",
    "gov_model = kmeans.fit(gov_embedding_vectors)\n",
    "gov_results = gov_model.transform(gov_embedding_vectors)\n",
    "\n",
    "#telephone\n",
    "tele_model = kmeans.fit(tele_embedding_vectors)\n",
    "tele_results = tele_model.transform(tele_embedding_vectors)\n",
    "\n",
    "#slate\n",
    "slate_model = kmeans.fit(slate_embedding_vectors)\n",
    "slate_results = slate_model.transform(slate_embedding_vectors)\n",
    "\n",
    "#fiction\n",
    "fiction_model = kmeans.fit(fiction_embedding_vectors)\n",
    "fiction_results = fiction_model.transform(fiction_embedding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_results.groupBy(\"prediction\").count().show()\n",
    "gov_results.groupBy(\"prediction\").count().show()\n",
    "tele_results.groupBy(\"prediction\").count().show()\n",
    "slate_results.groupBy(\"prediction\").count().show()\n",
    "fiction_results.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate cluster label name\n",
    "# cluter label name by maximum genre, if genre exists in other cluster, then choose the second mximum one (duplicate not allowed)\n",
    "'''\n",
    "from the above result, we conclude the cluster label for the row:\n",
    "cluster 0 = telephone\n",
    "cluster 1 = fiction\n",
    "cluster 2 = travel\n",
    "cluster 3 = slate\n",
    "cluster 4 = government\n",
    "\n",
    "In the confusion matrix, the column is as followed:\n",
    "true label 0 = travel\n",
    "true label 1 = slate\n",
    "true label 2 = fiction\n",
    "true label 3 = government\n",
    "true label 4 = telephone\n",
    "'''\n",
    "# column travel: total = 154700\n",
    "travel_1 = 31708 / 154700\n",
    "travel_3 = 25026 / 154700\n",
    "travel_4 = 26783 / 154700\n",
    "travel_2 = 39131 / 154700\n",
    "travel_0 = 32052 / 154700\n",
    "\n",
    "# column government: total = 154700\n",
    "gov_1 = 32314 / 154700\n",
    "gov_3 = 17367 / 154700\n",
    "gov_4 = 38390 / 154700\n",
    "gov_2 = 37254 / 154700\n",
    "gov_0 = 29375 / 154700\n",
    "\n",
    "# column telephone: total = 166696\n",
    "tele_1 = 21026 / 166696\n",
    "tele_3 = 37211 / 166696\n",
    "tele_4 = 38160 / 166696\n",
    "tele_2 = 30525 / 166696\n",
    "tele_0 = 39774 / 166696\n",
    "\n",
    "# column slate: total = 154612\n",
    "slate_1 = 38203 / 154612\n",
    "slate_3 = 37185 / 154612\n",
    "slate_4 = 31379 / 154612\n",
    "slate_2 = 24075 / 154612\n",
    "slate_0 = 23770 / 154612\n",
    "\n",
    "# column fiction: total = 154696\n",
    "fiction_1 = 37610 / 154696\n",
    "fiction_3 = 17164 / 154696\n",
    "fiction_4 = 40297 / 154696\n",
    "fiction_2 = 21912 / 154696\n",
    "fiction_0 = 37713 / 154696\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find a K-Means Cluster in TF-IDF\n",
    "\n",
    "sen1 = train_df.drop(\"sentence2\")\n",
    "sen2 = train_df.drop(\"sentence1\")\n",
    "combine_train_df = sen1.union(sen2).withColumnRenamed(\"sentence1\",\"sentence\")\n",
    "combine_train_df= combine_train_df.na.fill({'sentence': '' })\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(combine_train_df)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=300000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "kmeans = KMeans(featuresCol='features',k=5, seed=500)\n",
    "model = kmeans.fit(rescaledData)\n",
    "results = model.transform(rescaledData)\n",
    "\n",
    "results.filter(results[\"prediction\"]==0).groupBy(\"genre\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
